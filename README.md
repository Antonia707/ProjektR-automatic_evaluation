# ProjektR-automatic_evaluation

## Overview

This project is designed for automatic evaluation of answers generated by a language model using the BLEU, rouge and Distinct-N score metric. The evaluation is based on a reference set of human-generated answers provided in the 'answers.json' file.

## File Structure

- `answers.json`: Contains the reference set of human-generated answers.
- `llm-answers/`: Directory containing JSON files with answers generated by a language model.

## Setup

1. Install the required Python libraries:

   ```bash
   pip install nltk
